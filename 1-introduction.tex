%!TEX root = main.tex

\section{Introduction}
% \todo[inline]{DW: Shift focus from detecting bots vs human to detecting different types of bots to understand their strategy? activities?}
Over the past decade, social media platforms such as Twitter and Reddit have exploded in popularity. These platforms are public spaces where millions of people around the world interact with each other and form opinions. However, a problem that has emerged is the increasing prevalence and influence of \textit{trolls} and \textit{social bots} in social media. Online trolls are predominantly human or hybrid (semi-automated) user accounts who behave in a deceptive, destructive, and/or disruptive manner in a social setting on the Internet \citep{BUCKELS201497}.
On the other hand, social bots are largely automated systems that pose as humans and seek to influence human communication and manipulate public opinion at scale. Bots have caused recent attention and controversy during the 2016 US Election, where it was found that they were not only highly prevalent but also highly influential and ideologically driven \cite{FM7090,rizoiu2018debatenight,Kollanyi.2016.presidentialdebate}. 
Recent studies have shown that Russian interference during the 2016 US Election involved a combination of troll and bot accounts \citep{ferrara-et-al-trolls-ideology}, and that social media has been weaponised to spread state-sponsored propaganda and destabilise foreign politics \citep{broniatowski-et-al,ICWSM1613006}. %Similarly, political troll groups have recently been identified to play an important role in mobilising public support for Donald Trump's 2016 US Presidential campaign \citep{ICWSM1613006}.

% For instance, \cite{rizoiu2018debatenight} found that social bots during the 2016 1st US Presidential Debate were more active, had higher political engagement, and were on average 2.5 times more influential than the average human.

Several attempts have been made to accurately and reliably detect trolls and bots in social media. In particular, current state-of-the-art approaches use supervised machine learning to estimate the likelihood of a Twitter user being a bot \citep{davis2016botornot} or a troll on web forums \citep{mihaylov2015finding}.
%Current state-of-the-art approaches use supervised machine learning to estimate the likelihood of a Twitter user being a bot or troll, this has several limitations, including requiring vast amount of feature sets, which are often impossible to obtain in practice, and a decaying accuracy over time (as bot design changes and trolls modify their discursive strategies)~\citep{mihaylov2015finding}.
While most studies have been focusing on detection, recent studies show the existence of sub-types of trolls based on political ideology~\citep{boatwrighttroll,ferrara-et-al-trolls-ideology,2018who-let-the-trolls-out,stewart2018examining}, suggesting a sophisticated interplay between the trolls based on their roles to manipulate public opinion effectively.
%Instead, the intuition is that it may be possible to distinguish user types or identities (e.g., bot versus human, or `left troll' versus `right troll') based solely on differences and similarities in the sequential trace of text they generate over time.

In this paper, we tackle the problem of identifying and distinguishing the roles and strategies of Russian trolls on Twitter. Specifically, we develop and evaluate an approximate string matching method to quantify and differentiate the roles of Russian troll accounts using a novel variant of \textit{edit distance} suited to natural language data, which we call \textit{time-sensitive semantic edit distance}. 
The proposed method are based solely on differences and similarities in the sequential trace of text they generate over time and does not require extensive amount of information often required for the detection algorithms.
Conceptually, the idea for this approach extends from recent developments in social theory, which combine Actor-Network Theory and the 19th century work of Gabriel Tarde \citep{latour2012whole,latour2002gabriel}. In doing so, this paper contributes several important theoretical and empirical results.

% On the other hand, approximate string matching, that is, edit distance, is a good method to quantify the similarity of two strings. Since we attempt to distinguish social bots and trolls based on text features, we assume that both bots or trolls posting may have some special features, for example, using specific hashtags with a specific sequence. Thus, the distances between social bots are small, the distances between humans are small, but the distances between social bots and humans will be larger. However, it is insufficient to distinguish bots or trolls using only edit distance. For example, bots or trolls may post in a similar way but using different words. Thus, based on edit distance, we also attempt to take semantics into account. For example, if there are three sentences, ``I love music'', ``I hate music'' and ``I like music''. All of these three sentences are similar with respect to the string form, but we know that ``love'' and ``like'' are more similar as they have similar semantics, while ``hate'' has the opposite meaning. In this research, we attempt to reflect the semantic similarity as well as the form similarity of strings at the same time.

According to the overarching goals, there are several research problems in this study. First, to develop the semantic edit distance, we need to address how two text snippets are semantically close to each other to take into account not just textual representations but the underlying semantic meanings. While attempting to incorporate semantic of text, the semantic evolve rapidly with the emergence of new memes and slang. Therefore, we also need to address how the temporal difference between two strings affects the textual closeness. Finally, we investigate the feasibility of such an approach and quantitatively describe the error rate of distinguishing different types of Russian trolls using such an approach, based on ground truth roles of the accounts (right versus left ideology, or news feed trolls).

This study is motivated both by a combination of computer science problems (developing new methods and results in the area of approximate string matching via edit distance), and social science problems (analysing and detecting bots and trolls on social media, using approximate string matching methods), as well as fundamental problems in social theory relating to identity and social order.

The main contributions of this work include:
\begin{itemize}
	\item We introduce a classification framework to identify the role of trolls through operationalisation of social theory on monadology;
	\item We develop a time-sensitive semantic edit distance to measure similarity between two tweets accounting semantic and temporal differences;
	\item We propose a visualisation based on the novel distance metric to gain new insights into the strategic behaviour of trolls around the 2016 U.S. presidential election.
\end{itemize}
