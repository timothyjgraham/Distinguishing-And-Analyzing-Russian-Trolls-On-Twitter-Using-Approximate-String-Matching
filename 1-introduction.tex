%!TEX root = main.tex

\section{Introduction}
Over the past decade, social media platforms such as Twitter and Reddit have exploded in popularity. These platforms are public spaces where millions of people around the world interact with each other and form opinions. However, a problem that has emerged is the increasing prevalence of \textit{social bots} and \textit{trolls} in social media. Social bots are automated or semi-automated systems that pose as humans and seek to influence human communication and manipulate public opinion at scale. Bots have caused recent attention and controversy during the 2016 US Election, where it was found that they were not only highly prevalent but also highly influential and ideologically driven \cite{FM7090,rizoiu2018debatenight,Kollanyi.2016.presidentialdebate}. On the other hand, online trolls are predominantly human or hybrid (semi-automated) user accounts who behave in a deceptive, destructive, and/or disruptive manner in a social setting on the Internet \citep{BUCKELS201497}. Recent studies have shown that Russian interference during the 2016 US Election involved a combination of bot and troll accounts, and that social media continues to be weaponised to spread state-sponsored propaganda and destabilise foreign politics \citep{broniatowski-et-al}. Similarly, political troll groups have recently been identified to play an important role in mobilising public support for Donald Trump's 2016 US Presidential campaign \citep{ICWSM1613006}.

% For instance, \cite{rizoiu2018debatenight} found that social bots during the 2016 1st US Presidential Debate were more active, had higher political engagement, and were on average 2.5 times more influential than the average human.

An ongoing problem is how to accurately and reliably detect bots and trolls in social media, in particular on Twitter. Current state-of-the-art approaches use supervised machine learning (ML) to estimate the likelihood of a Twitter user being a bot (or not), but as discussed in SECTION X, this has several limitations, including costly annotation of training data, diminished accuracy in particular contexts (e.g. political discussions versus general Twitter discussions), and a decaying accuracy over time (as bot design changes and trolls modify their discursive strategies). Similar approaches have been used for \textit{troll detection}, which rely on supervised ML and manually annotated training datasets \citep{mihaylov2015finding}. However, there is almost no research that attempts to detect social bots or trolls using \textit{unsupervised} ML. Compared with supervised approaches, unsupervised ML does not require large amounts of annotated training data. Instead, the intuition is that it may be possible to distinguish user types or identities (e.g., bot versus human, or `left troll' versus `right troll') based solely on differences and similarities in the sequential trace data they generate over time (e.g., text features extracted from tweets). 

Therefore, in this paper we develop and evaluate an approximate string matching method to quantify and differentiate user identities using a novel variant of \textit{edit distance} suited to natural language data, which we call \textit{semantic edit distance}. We apply this method to automatically distinguish different types of Russian trolls on Twitter. Conceptually, the idea for this approach extends from recent developments in social theory, which combine Actor-Network Theory and the 19th century work of Gabriel Tarde \citep{latour2012whole, latour2002gabriel}. In doing so, this paper contributes several important theoretical and empirical results.

% On the other hand, approximate string matching, that is, edit distance, is a good method to quantify the similarity of two strings. Since we attempt to distinguish social bots and trolls based on text features, we assume that both bots or trolls posting may have some special features, for example, using specific hashtags with a specific sequence. Thus, the distances between social bots are small, the distances between humans are small, but the distances between social bots and humans will be larger. However, it is insufficient to distinguish bots or trolls using only edit distance. For example, bots or trolls may post in a similar way but using different words. Thus, based on edit distance, we also attempt to take semantics into account. For example, if there are three sentences, ``I love music'', ``I hate music'' and ``I like music''. All of these three sentences are similar with respect to the string form, but we know that ``love'' and ``like'' are more similar as they have similar semantics, while ``hate'' has the opposite meaning. In this research, we attempt to reflect the semantic similarity as well as the form similarity of strings at the same time.

% This research project has two main components. The first involves the development of an algorithm for semantic edit distance (herein \textit{SED}), which is a novel variant of edit distance that is tailored to string sequences where the symbols are natural language (in this case words, n-grams, and hashtags extracted from tweet text), and therefore have latent semantic structure that is not captured by existing methods. Secondly, we apply this method to analyse Russian Twitter trolls and attempt to distinguish their type (e.g., `right' troll versus `left' troll) in an unsupervised fashion using only tweet text, and evaluating the feasibility and performance of the algorithm. 

According to the overarching goals, there are several research problems in this study. First, to develop the semantic edit distance, we need to address how the costs for semantic distance can be included in the Levenshtein edit distance algorithm, as well as how to obtain the semantic similarity. Furthermore, while we attempt to use edit distance and a novel semantic edit distance to cluster Twitter users based on what they write about (e.g., tweet text), we need to find an appropriate method to normalize the distance results. Finally, we investigate the feasibility of such an approach and qualitatively describe the error rate of distinguishing different types of Russian trolls using such an approach, based on ground truth labels of political ideology (left versus right).

This study is motivated both by a combination of computer science problems (developing new methods and results in the area of approximate string matching via edit distance), and social science problems (analyzing and detecting bots and trolls on social media, using unsupervised string matching methods), as well as fundamental problems in social theory relating to identity and social order.

\textbf{The main contributions of this work include:}
%\squishlist
\begin{itemize}
	\item to do

	\item to do
	
	\item to do
		
	\item to do
\end{itemize}

The remainder of this research report is structured as follows. TO DO.