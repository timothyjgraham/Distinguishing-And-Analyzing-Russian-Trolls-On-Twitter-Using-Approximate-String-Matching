%!TEX root = main.tex

\section{Background}

We structure the discussion of background work into three categories: (1) related work on social media trolls and social bots; (2) a theoretical framework for measuring `identity' using social media data; and (3) work relating to edit distance and approximate string matching. 

\subsection{Political bots and trolls online}
%, applied to to the 2016 U.S. election.
\textbf{Social Bots and Trolls.} In recent years social bots and online trolls have attracted considerable scholarly attention. Recent studies have investigated the impact and influence of bots in social media \citep{aiello2012people}. Social bots have been shown to influence public opinion , spread fake news \citep{shao2017spread}, and affect the finance and stock market \citep{ferrara2016rise}. There has also been much research on general troll behavior, slightly similar to social bots, trolls tend to provoke others and draw people into arguments and diverting attention \citep{herring2002searching}. \cite{buckels2014trolls} also indicate that there is a robust association with trolling behavior and sadism.

\textbf{Detecting bots and trolls on social media}. While social bots and trolls in social media become highly prevalent and influential, the methods to detect them in social networks has also received wide attention \citep{cook2014twitter}. One of the most popular and state-of-the-art methods to detect social bots is to use BotOrNot API \citep{davis2016botornot}. It uses Random Forest, which is an ensemble supervised learning method, to measure the likelihood of a user being a bot based on more than 1000 features extracted from meta-data, patterns of activity, and tweet content \citep{varol2017online}, and can achieve a performance of 0.95 AUC (Area Under ROC Curve). Besides, \cite{morstatter2016new} propose a bot detection model which considers recall in the formulation and achieve a better performance than the heuristics and topic modeling baseline methods. \cite{ferrara2016rise} also mention many other detection methods such as detection based on social network information and based on crowdsourcing and leveraging human intelligence. However, all of these methods require the costly annotation of training data, and requires many attributes such as friends, followers, content and sentiment of tweet, social network patterns, and activity time series \citep{varol2017online}. 

On the other hand, detecting trolls is a hard problem, as there is even no unified definition of what they are, and it also relies on supervised machine learning and manually annotated training datasets \citep{mihaylov2015finding}. Moreover, in addition to the general troll, there are also some trolls employed by some specific agencies to achieve specialized goals, such as Russian Trolls, which is employed by Russia's Internet Research Agency to influence the politics and public thoughts of the United States \citep{boatwrighttroll}. The Clemson researchers \cite{boatwrighttroll} use advanced tracking software of social media to pull the tweets from a large number of accounts that Twitter has acknowledged as being related with the IRA. The researchers first qualitative analyze the data then use quantitative analysis to explore how behavior changes over time. However, it is quite time-consuming and cannot detect trolls automatic as it requires people to manual annotation. 

Therefore, in this research, our methods aim to address a gap in the current research, namely an unsupervised approach using a novel edit distance algorithm that only requires a comparatively small amount of information (tweet text) compared to current supervised methods that are resource intensive. However, before proceeding further it is first necessary to define and conceptualise what we mean by `identity', including how we propose to quantify and analyse it using approximate string matching techniques.

\subsection{Quantifying `identity' using online trace data}

Gabriel Tarde's ancient theory of monadology (Tarde, 2012 [1895]) has recently been adapted into the body of social theory known as Actor-Network Theory (ANT). It promises a powerful framework for the study of identity and social change in heterogeneous networks (Latour et al, 2012). In the 19th century, Tarde's ideas proved not only difficult to conceptualise but even more difficult to put into practice as an analytical approach due to a lack of available data. It is perhaps for this reason that Tarde's alternative approach to describing social processes was not empirically testable and subsequently relegated to a footnote in history.

However, Latour et al. (2012) argue that the onset of the information age and the availability of digital data sets make it possible to revisit Tardeâ€™s ideas and render them practical. By examining the digital traces left behind by actors in a network (human and non-human), Latour et al. (2012: 598) argue that we can `slowly learn about what an entity ``is'' by adding more and more items to its profile'. The radical conclusion is that datasets `allow entities to be individualized by the never-ending list of particulars that make them up' (Latour et al. 2012: 600). Hence, a monad is a `point of view, or, more exactly, a type of navigation that composes an entity through other entities' (Latour et al. 2012: 600).

As an example of this form of analysis, Latour et al. (2012) use the example of looking up an academic named `Herve C.' on the web to show how collecting information through various digital sources results in the assemblage of a network that defines an actor's identity. As the authors argue: `The set of attributes - the network - may now be grasped as an envelope - the actor - that encapsulates its content in one shorthand notation' (Latour et al. 2012: 593). Instead of atomic nodes (micro) that somehow `enter into' or `end up forming' structures (macro), we have a very different view of identity: in order to know what something is, we simply follow the traces that it leaves behind through its relations to other entities:

\begin{quote}
If for instance we look on the web for the curriculum vitae of a scholar we have never heard of before, we will stumble on a list of items that are at first vague. Let's say that we have been just told that `Herve C.' is now `professor of economics at Paris School of Management'. At the start of the search it is nothing more than a proper name. Then, we learn that he has a `PhD from Penn University', `has written on voting patterns among corporate stake holders',`has demonstrated a theorem on the irrationality of aggregation', etc. If we go on through the list of attributes, the definition will expand until paradoxically it will narrow down to a more and more particular instance. Very quickly, just as in the kid game of Q and A, we will zero in on one name and one name only, for the unique solution: `Herve C.'. Who is this actor? Answer: this network. What was at first a meaningless string of words with no content, a mere dot, now possesses a content, an interior, that is, a network summarized by one now fully specified proper name (Latour et al., 2012: 592).
\end{quote}

To summarise, what Latour et al (2012) appear to suggest is that we can replace the idea of identity in terms of individual attributes (e.g., gender, age, location, etc), and focus instead on defining entities based on the traces they leave behind. This implies that two users might be similar because the trace data they leave behind (e.g., words and hashtags used in tweet texts) have a large number of elements in common. However, whilst such an approach is interesting, it also has the limitation that it does not preserve the order in which events occur. For example, a musical song can be decomposed as a set of notes, however the order that these notes occur gives form to the song. Similarly, if the identity of `Herve C' as mentioned previously is traced via a series of web searches, it is important that `PhD from Penn University' occurred \textit{before} `professor of economics at Paris School of Management', such that the sequence of events provides important information about the identity of this individual unfolding over time.

In the case of Twitter data and identifying Russian troll accounts, it is easy to see that for a given `right troll' $u_i \in U$, their `monadic' identity can be described as a vector $\vec v$, which is a sequence of actions or events initiated by $u_i$, ordered by time. As a simplifying example, if we extract the hashtags that $u_i$ has used in tweets, we might find that they used \textit{\#MAGA} at time $t_0$, \textit{\#crookedhillary} at $t_1$, and \textit{\#clintonemails} at $t_2$. Thus the sequence $\vec v_i$ expressing their identity is $\left<\#MAGA, \#crookedhillary, \#clintonemails\right>$. Similarly, another right troll $u_j$ might have the following monadic identity $\vec v_j$: $\left<\#clintonemails, \#MAGA, \#trump2016\right>$, whilst a `left troll' $u_k$ authored the following sequence of hashtags $\vec v_k$: $\left<\#blacklivesmatter, \#bernieorbust, \#feelthebern\right>$. Although this example is purely illustrative, it is clear that the monadic identities of the right trolls, encoded as $\vec v_i$ and $\vec v_j$, are more similar compared to the left troll, encoded as $\vec v_k$. The question that we pose in this paper is whether we can exploit this approach to encoding identity in order to cluster users based on similarities in their trace activity over time. In the next section we show how this is possible when we make some modifications to an approximate string matching technique known as edit distance.

\subsection{Edit distance}

The edit distance (ED) is a method of quantifying the similarity of two strings by calculating the minimum value of the required edit operations to transform one string into the other. There may be different sets of operations according to the definition of ED. The most common form of ED is known as Levenshtein distance. In Levenshtein's original definition, the edit operations include the insertion, deletion and substitution of a symbol in the string, and each of these operations has a unit cost, so the original edit distance is equal to the minimum number of the required operations to transform one string into the other. 

\textbf{Application of edit distance.} Edit distance has a wide range of applications, especially in information retrieval, natural language processing and computational biology \citep{navarro2001guided}. In terms of natural language processing, edit distance is applied to the correction of spelling mistakes or OCR (Optical Character Recognition) errors. \cite{arora2010recognition} combine neural networks and edit distance to recognize non-compound handwritten Devnagari characters and achieve a high recognition at 90.74\%. In OCR application, the edit distance is used to obtain the minimum number of the editing operations needed to transform the wrong word to the right word in the dictionary. In order to correct the wrong words, we reserve a set of solutions that require fewer editing operations. In computational biology, DNA and protein sequences can be seen as long strings over specific alphabets, which stores biological information representing the genetic code of living beings. It is fundamental and essential to search and comparing specific sequences over those strings. 
 
Edit distance also has several limitations. In particular, it can only quantify the similarity of the structure of strings composed of symbols in an alphabet $\Sigma$, while not considering the semantic space in which the elements of $\Sigma$ occur. This is a limitation because while detecting social bots or trolls using edit distance, we want to take some contextual factors into account, in this case, the latent semantic structure of the input data (natural language expressed in tweets). For example, consider the following three sentences: ``I love music''; ``I hate music''; and ``I like music''. Every word can be considered as a symbol of some alphabet $\Sigma$, thus the edit distance between each pair of two sentences is 1. However, ``love'' is more semantically similar to ``like''. In this situation, the distance between ``I love music'' and ``I like music'' should be less than the distance between ``I love music'' and ``I hate music''. 

It follows that if we wish to use edit distance to distinguish between the identity of Twitter users using unsupervised clustering techniques, then it is critical to take semantic similarity into account. This is the problem that we address in Section X. Before we tackle this problem, the next section provides details of the Russian Troll Dataset as well as the types of trolls that we aim to distinguish between using an unsupervised troll detection approach.