%!TEX root = main.tex
%\newpage

\section{Normalised SED}
\label{sec:resultnorm}

Both edit distances and semantic edit distances ranges between zero and infinity in theory, but the actual distances are bounded by the length of the sequences in practice. Both distances will not be greater than the maximum length of sequences, therefore, if the two sequences are short, the distances will be small even they are totally different sequences. 
Thus, to eliminate the impact of the length of tweets on distance, we propose and test two different normalisation methods.

First, we use the ratio between the semantic edit distance and edit distance as follows 
\begin{equation}
  \text{SED/ED}(\boldsymbol{a}, \boldsymbol{b})=\frac{\sed(\boldsymbol{a}, \boldsymbol{b})}{\ed(\boldsymbol{a}, \boldsymbol{b})}.
\end{equation}
Note that the semantic edit distance is always less than the edit distance with our definition of cost function, therefore the ratio also ranges between zero and one. If the ratio is close to zero, then it indicates that there are differences between the textual representations of both sequences, however, they are semantically very similar.

Second, we use the maximum length of the sequences to normalise the semantic edit distance as follows
\begin{equation}
  \text{SED/Max}(\boldsymbol{a}, \boldsymbol{b})=\frac{\sed(\boldsymbol{a}, \boldsymbol{b})}{\max(|\boldsymbol{a}|, |\boldsymbol{b}|)}.
\end{equation}
In the worse case, the semantic edit distance will be the same as the maximum length of sequences, thus this normalisation also ranges between zero and one. Intuitively, this metric measures an average cost of operation on a single symbol in a sequence.

